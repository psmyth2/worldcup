{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WorldCup_labeled = pd.read_csv('WCSemi_Sentiment_Labeled.csv', encoding='latin1', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>screenName</th>\n",
       "      <th>userId</th>\n",
       "      <th>text</th>\n",
       "      <th>location</th>\n",
       "      <th>multi-team</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Shrupti</td>\n",
       "      <td>6.088599e+07</td>\n",
       "      <td>Its coming home #England https://t.co/jGuIRBj46I</td>\n",
       "      <td>Arlington, VA</td>\n",
       "      <td>False</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>PropSwap</td>\n",
       "      <td>2.862105e+09</td>\n",
       "      <td>SOLD! This Croatia to Win the #WorldCup ticket...</td>\n",
       "      <td>Enterprise, NV</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>MentorPlanet</td>\n",
       "      <td>8.650257e+07</td>\n",
       "      <td>#France is pledging 1.5 billion pure governme...</td>\n",
       "      <td>Minneapolis, MN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>NFLMarquise</td>\n",
       "      <td>9.080000e+17</td>\n",
       "      <td>.@MDbankroll Thanks to you for being my 300th ...</td>\n",
       "      <td>Moon, PA</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>FassiCarlo</td>\n",
       "      <td>9.152236e+08</td>\n",
       "      <td>.@GianluigiBuffon says France is the most ser...</td>\n",
       "      <td>Winter Haven, FL</td>\n",
       "      <td>False</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Caitlin11790</td>\n",
       "      <td>9.160152e+08</td>\n",
       "      <td>People at my work are emailing and calling ask...</td>\n",
       "      <td>Clinton, MI</td>\n",
       "      <td>False</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>faustaLV</td>\n",
       "      <td>2.603407e+08</td>\n",
       "      <td>??Pure moment of love ??#motherandson #love #m...</td>\n",
       "      <td>Las Vegas, NV</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Miiikey</td>\n",
       "      <td>1.549559e+07</td>\n",
       "      <td>@DannyWelbeck Welbz, how many retweets for a W...</td>\n",
       "      <td>San Diego, CA</td>\n",
       "      <td>False</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>TheRealPolyG</td>\n",
       "      <td>5.390822e+08</td>\n",
       "      <td>These World Snooze off days have been throwing...</td>\n",
       "      <td>Philadelphia, PA</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>vivathematadors</td>\n",
       "      <td>1.792236e+07</td>\n",
       "      <td>We made it #worldcup https://t.co/tIxmyhAbGz</td>\n",
       "      <td>Lubbock, TX</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id       screenName        userId  \\\n",
       "0   0          Shrupti  6.088599e+07   \n",
       "1   1         PropSwap  2.862105e+09   \n",
       "2   2     MentorPlanet  8.650257e+07   \n",
       "3   3      NFLMarquise  9.080000e+17   \n",
       "4   4       FassiCarlo  9.152236e+08   \n",
       "5   5     Caitlin11790  9.160152e+08   \n",
       "6   6         faustaLV  2.603407e+08   \n",
       "7   7          Miiikey  1.549559e+07   \n",
       "8   8     TheRealPolyG  5.390822e+08   \n",
       "9   9  vivathematadors  1.792236e+07   \n",
       "\n",
       "                                                text          location  \\\n",
       "0  Its coming home #England https://t.co/jGuIRBj46I     Arlington, VA   \n",
       "1  SOLD! This Croatia to Win the #WorldCup ticket...    Enterprise, NV   \n",
       "2  #France is pledging 1.5 billion pure governme...   Minneapolis, MN   \n",
       "3  .@MDbankroll Thanks to you for being my 300th ...          Moon, PA   \n",
       "4  .@GianluigiBuffon says France is the most ser...  Winter Haven, FL   \n",
       "5  People at my work are emailing and calling ask...       Clinton, MI   \n",
       "6  ??Pure moment of love ??#motherandson #love #m...     Las Vegas, NV   \n",
       "7  @DannyWelbeck Welbz, how many retweets for a W...     San Diego, CA   \n",
       "8  These World Snooze off days have been throwing...  Philadelphia, PA   \n",
       "9       We made it #worldcup https://t.co/tIxmyhAbGz       Lubbock, TX   \n",
       "\n",
       "   multi-team sentiment  \n",
       "0       False       pos  \n",
       "1       False       NaN  \n",
       "2       False       NaN  \n",
       "3       False       NaN  \n",
       "4       False       pos  \n",
       "5       False       pos  \n",
       "6       False       NaN  \n",
       "7       False       pos  \n",
       "8       False       NaN  \n",
       "9       False       NaN  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WorldCup_labeled.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets labeled positive: 544\n",
      "Number of tweets labeled negative: 15\n"
     ]
    }
   ],
   "source": [
    "#Use list comprehensions to make lists of positive and negative tweets\n",
    "pos_tweets = [(WorldCup_labeled.loc[row,'text'],'positive') for row in range(len(WorldCup_labeled)) if \\\n",
    "              WorldCup_labeled.loc[row,'sentiment'] == 'pos']\n",
    "\n",
    "neg_tweets = [(WorldCup_labeled.loc[row,'text'],'negative') for row in range(len(WorldCup_labeled)) if \\\n",
    "              WorldCup_labeled.loc[row,'sentiment'] == 'neg']\n",
    "\n",
    "print('Number of tweets labeled positive: %d' % len(pos_tweets))\n",
    "print('Number of tweets labeled negative: %d' % len(neg_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#half the negative tweets go in training\n",
    "#Downsampling the positive tweets at 1 pos:1 neg\n",
    "len_train = int(round(len(neg_tweets)/2)*2)\n",
    "train_tweets = neg_tweets[:int(len_train/2)] + pos_tweets[:int(len_train/2)]\n",
    "\n",
    "#half of the remaining half go in cv\n",
    "cv_neg_cutoff = int( (len_train/2) + round((len(neg_tweets) - len_train/2)/2) )\n",
    "cv_pos_cutoff = int( (len_train/2) + round((len(pos_tweets) - len_train/2)/2) )\n",
    "cv_tweets =  neg_tweets[int(len_train/2):cv_neg_cutoff] +  pos_tweets[int(len_train/2):cv_pos_cutoff]  \n",
    "\n",
    "#rest go into testing\n",
    "test_tweets = neg_tweets[cv_neg_cutoff:] +  pos_tweets[cv_pos_cutoff:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\patrick.smyth.TPLGIS\\AppData\\Roaming\\nltk_dat\n",
      "[nltk_data]     a...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.util import mark_negation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'do', 'not', 'like_NEG', '#England_NEG']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = \"I do not like #England\".split()\n",
    "nltk.sentiment.util.mark_negation(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.metrics import BigramAssocMeasures\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "import string\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Set to exclude punctuation marks\n",
    "exclude = set(string.punctuation)\n",
    "\n",
    "#List to exclude words that can identify the team from list of features\n",
    "excluded_words = ['england','belgium','france','croatia',\\\n",
    "                  'eng','bel','fra','cro',\\\n",
    "                  'itscominghome','lions','bleus','devils','blues']\n",
    "\n",
    "#Function that provides a list of filtered unigrams and bigrams from each tweet\n",
    "def filter_tweets(tweets):\n",
    "    filtered_tweets = []\n",
    "    \n",
    "    #Get a list of words, and the sentiment for each tweet\n",
    "    for (words, sentiment) in tweets: \n",
    "        words_filtered=[]\n",
    "        \n",
    "        #For each word in the list of words, filter on our feature requirements. \n",
    "        for word in words.split(): \n",
    "            \n",
    "            #Remove punctuation\n",
    "            word = ''.join(ch for ch in word if ch not in exclude)\n",
    "\n",
    "            #Remove zero letter \"words\"\n",
    "            if len(word) >= 1: \n",
    "                \n",
    "                    #treat URLs the same\n",
    "                    if word[:4] == 'http':\n",
    "                        word='http'\n",
    "                        \n",
    "                    #remove hashtags\n",
    "                    if word[0] == '#': \n",
    "                        word=word[1:]\n",
    "                        \n",
    "                    #remove team identifiers\n",
    "                    if (word.lower() not in excluded_words):\n",
    "        \n",
    "                        #require lower case\n",
    "                        words_filtered.append(word.lower()) \n",
    "\n",
    "        #Identify top 200 bigams in the filtered word list using chi_sq measure of importance\n",
    "        bigram_finder = BigramCollocationFinder.from_words(words_filtered)\n",
    "        bigrams = bigram_finder.nbest(BigramAssocMeasures.chi_sq, 200)      \n",
    "\n",
    "        #Add the final bigrams and unigrams for this tweet to the filtered list\n",
    "        filtered_tweets.append(([ngram for ngram in itertools.chain(words_filtered, bigrams)],sentiment))\n",
    "\n",
    "    #Return the filtered list for all tweets\n",
    "    return filtered_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Filter each set of data\n",
    "train_tweets = filter_tweets(train_tweets)\n",
    "cv_tweets = filter_tweets(cv_tweets)\n",
    "test_tweets = filter_tweets(test_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['gianluigibuffon',\n",
       "  'says',\n",
       "  '\\x93france',\n",
       "  'is',\n",
       "  'the',\n",
       "  'most',\n",
       "  'serious',\n",
       "  'semifinalist',\n",
       "  'capable',\n",
       "  'of',\n",
       "  'winning',\n",
       "  'the',\n",
       "  'fifaworldcup\\x94',\n",
       "  'based',\n",
       "  'on',\n",
       "  'thei',\n",
       "  'http',\n",
       "  ('based', 'on'),\n",
       "  ('capable', 'of'),\n",
       "  ('fifaworldcup\\x94', 'based'),\n",
       "  ('gianluigibuffon', 'says'),\n",
       "  ('most', 'serious'),\n",
       "  ('of', 'winning'),\n",
       "  ('on', 'thei'),\n",
       "  ('says', '\\x93france'),\n",
       "  ('semifinalist', 'capable'),\n",
       "  ('serious', 'semifinalist'),\n",
       "  ('thei', 'http'),\n",
       "  ('\\x93france', 'is'),\n",
       "  ('is', 'the'),\n",
       "  ('the', 'fifaworldcup\\x94'),\n",
       "  ('the', 'most'),\n",
       "  ('winning', 'the')],\n",
       " 'positive')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tweets[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function that builds a list of features from the list of unigrams and bigrams\n",
    "#Requires each unigram or bigram to show up some minimum number of times to be considered a feature\n",
    "def get_word_features(tweets,min_freq):\n",
    "\n",
    "    #Create a list of ALL unigrams and bigrams\n",
    "    wordlist = []\n",
    "    for (words, sentiment) in tweets:\n",
    "        wordlist.extend(words)\n",
    "    \n",
    "    #Count the frequency of each unigram and bigram\n",
    "    wordlist = nltk.FreqDist(wordlist)\n",
    "    \n",
    "    #Sort the list of unigrams and bigrams based on frequency\n",
    "    sorted_word_list = sorted(wordlist.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    #Only include the unigrams and bigrams as features if they appear at least min_freq times\n",
    "    word_features = [sorted_word_list[word][0] for word in \\\n",
    "    \trange(len(sorted_word_list)) if sorted_word_list[word][1] >= min_freq]\n",
    "    \n",
    "    #Return the list of features\n",
    "    return word_features\n",
    "\n",
    "word_features = get_word_features(train_tweets,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Feature extractor - determines which word features are in each tweet\n",
    "def extract_features(filtered_tweet):\n",
    "\n",
    "    #list of unigrams and bigrams in the tweet\n",
    "    filtered_tweet_words = set(filtered_tweet)\n",
    "    \n",
    "    #Define a features dictionary\n",
    "    features = {}\n",
    "\n",
    "    #Loop of all word features\n",
    "    for word in word_features:\n",
    "        \n",
    "        #Set 'contains(word_feature)' as a key in the dictionary\n",
    "        #Set the value for that key to True or False\n",
    "        features['contains(%s)' % str(word)] = (word in filtered_tweet_words)\n",
    "\n",
    "    #Return the final features dictionary for that tweet\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Extract features from each tweets\n",
    "training_set = nltk.classify.apply_features(extract_features, train_tweets)\n",
    "cv_set = nltk.classify.apply_features(extract_features, cv_tweets)\n",
    "test_set = nltk.classify.apply_features(extract_features, test_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"contains(('coming', 'home'))\": False,\n",
       " \"contains(('worldcup', 'http'))\": False,\n",
       " 'contains(a)': True,\n",
       " 'contains(coming)': False,\n",
       " 'contains(fans)': False,\n",
       " 'contains(home)': False,\n",
       " 'contains(http)': False,\n",
       " 'contains(is)': True,\n",
       " 'contains(of)': False,\n",
       " 'contains(on)': False,\n",
       " 'contains(the)': False,\n",
       " 'contains(win)': False,\n",
       " 'contains(world)': False,\n",
       " 'contains(worldcup)': False}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_number=1\n",
    "training_set[tweet_number][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Train the classifier\n",
    "classifier = nltk.NaiveBayesClassifier.train(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.metrics import precision as prec\n",
    "from nltk.metrics import recall as rec\n",
    "from nltk.metrics import f_measure as fmeas\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to evaluate our classifier on the given subset of data\n",
    "def eval_classifier(data_set):\n",
    "\n",
    "    #Use .accuracy method to calculate accuracy\n",
    "    cross_valid_accuracy = nltk.classify.accuracy(classifier, data_set)\n",
    "\n",
    "    #Create two sets which we'll use to count positive and negative tweets\n",
    "    ref_set = collections.defaultdict(set)\n",
    "    obs_set = collections.defaultdict(set)\n",
    "\n",
    "\n",
    "    #Loop over each tweet in our cross validation set\n",
    "    for i, (feats, label) in enumerate(data_set):\n",
    "\n",
    "        #Classify the tweet by feeding the classifier the tweet's features\n",
    "        observed = classifier.classify(feats)\n",
    "\n",
    "        #Add the current tweet to the \"reference\" set under the actual class\n",
    "        ref_set[label].add(i)\n",
    "\n",
    "        #Add the current tweet to the \"observation\" set under the predicted class\n",
    "        obs_set[observed].add(i)\n",
    "\n",
    "\n",
    "    #Calculate F score, precision, an recall for positive and negative labels\n",
    "    #Also calculate accuracy and NTC improvement\n",
    "    print ('Accuracy:', cross_valid_accuracy)\n",
    "    print ('F-measure [negative]:', fmeas(ref_set['negative'], obs_set['negative']))\n",
    "    print ('F-measure [positive]:', fmeas(ref_set['positive'], obs_set['positive']))\n",
    "    print ('Precision [negative]:', prec(ref_set['negative'], obs_set['negative']))\n",
    "    print ('Precision [positive]:', prec(ref_set['positive'], obs_set['positive']))\n",
    "    rec_neg=rec(ref_set['negative'], obs_set['negative'])\n",
    "    rec_pos=rec(ref_set['positive'], obs_set['positive'])\n",
    "    print ('Recall [negative]:', rec_neg)\n",
    "    print ('Recall [positive]:', rec_pos)\n",
    "    total_neg=len(neg_tweets)\n",
    "    total_pos=len(pos_tweets)\n",
    "    ntc_improvement = 100*((total_pos + total_neg)/total_neg)*( (total_neg/(total_neg+total_pos)) - \\\n",
    "    \t(total_neg*(1-rec_neg))/(total_neg*(1-rec_neg) + total_pos*rec_pos))\n",
    "    print ('Negative contamination improved by ', ntc_improvement, 'percent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8860294117647058\n",
      "F-measure [negative]: 0.06060606060606061\n",
      "F-measure [positive]: 0.939334637964775\n",
      "Precision [negative]: 0.034482758620689655\n",
      "Precision [positive]: 0.9876543209876543\n",
      "Recall [negative]: 0.25\n",
      "Recall [positive]: 0.8955223880597015\n",
      "Negative contamination improved by  15.88321167883211 percent\n"
     ]
    }
   ],
   "source": [
    "eval_classifier(cv_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "contains(('worldcup', 'http')) = True           negati : positi =      3.0 : 1.0\n",
      "          contains(fans) = False          positi : negati =      2.4 : 1.0\n",
      "            contains(is) = True           positi : negati =      2.3 : 1.0\n",
      "      contains(worldcup) = False          positi : negati =      1.8 : 1.0\n",
      "            contains(on) = True           positi : negati =      1.7 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Show the 5 most important features of our classifier\n",
    "print (classifier.show_most_informative_features(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "#Save the classifier for later use\n",
    "f = open('WorldCup_tweet_classifier.pickle', 'wb')\n",
    "pickle.dump(classifier, f)\n",
    "f.close()\n",
    "\n",
    "#Save document_words as well\n",
    "with open('WorldCup_classifier_feats.pickle', 'wb') as f:\n",
    "    pickle.dump(word_features, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9188191881918819\n",
      "F-measure [negative]: 0\n",
      "F-measure [positive]: 0.9576923076923076\n",
      "Precision [negative]: 0.0\n",
      "Precision [positive]: 0.9880952380952381\n",
      "Recall [negative]: 0.0\n",
      "Recall [positive]: 0.9291044776119403\n",
      "Negative contamination improved by  -7.410593937308235 percent\n"
     ]
    }
   ],
   "source": [
    "#Evaluating classifier on test set\n",
    "eval_classifier(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original systematic error from uncut negative tweets was:  2.68 percent\n",
      "Improved systematic error from uncut negative tweets is:  0.77 percent\n"
     ]
    }
   ],
   "source": [
    "#Calculating initial and final negative contamination\n",
    "total_neg=len(neg_tweets)\n",
    "total_pos=len(pos_tweets)\n",
    "\n",
    "print('Original systematic error from uncut negative tweets was: ', \\\n",
    "      round(10000*(total_neg/(total_neg + total_pos)))/100, 'percent')\n",
    "      \n",
    "print('Improved systematic error from uncut negative tweets is: ', \\\n",
    "      round((1-0.714)*10000*(total_neg/(total_neg + total_pos)))/100, 'percent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
